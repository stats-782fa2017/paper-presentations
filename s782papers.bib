%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Daniel McDonald at 2017-08-13 11:26:48 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{pmlr-v54-kpotufe17a,
	Abstract = {Density-ratio estimation (i.e. estimating $f = f_Q/f_P$ for two unknown distributions Q and P) has proved useful in many Machine Learning tasks, e.g., risk-calibration in transfer-learning, two-sample tests, and also useful in common techniques such importance sampling and bias correction. While there are many important analyses of this estimation problem, the present paper derives convergence rates in other practical settings that are less understood, namely, extensions of traditional Lipschitz smoothness conditions, and common high-dimensional settings with structured data (e.g. manifold data, sparse data).   Various interesting facts, which hold in earlier settings, are shown to extend to these settings. Namely, (1) optimal rates depend only on the smoothness of the ratio f, and not on the densities $f_Q$, $f_P$, supporting the belief that plugging in estimates for $f_Q$, $f_P$ is suboptimal; (2) optimal rates depend only on the intrinsic dimension of data, i.e. this problem -- unlike density estimation -- escapes the curse of dimension.   We further show that near-optimal rates are attainable by estimators tuned from data alone, i.e. with no prior distributional information. This last fact is of special interest in unsupervised settings such as this one, where only oracle rates seem to be known, i.e., rates which assume critical distributional information usually unavailable in practice. },
	Address = {Fort Lauderdale, FL, USA},
	Author = {Samory Kpotufe},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:25:48 +0000},
	Date-Modified = {2017-08-13 15:25:48 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {1320--1328},
	Pdf = {http://proceedings.mlr.press/v54/kpotufe17a/kpotufe17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Lipschitz Density-Ratios, Structured Data, and Data-driven Tuning}},
	Url = {http://proceedings.mlr.press/v54/kpotufe17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/kpotufe17a.html}}

@inproceedings{pmlr-v54-sankaran17a,
	Abstract = {The failure of LASSO   to identify groups of correlated predictors in linear  regression  has sparked significant research interest. Recently,   various norms were proposed, which can be  best described as instances of ordered weighted $\ell_1$ norms (OWL), as an alternative to $\ell_1$ regularization used in LASSO. OWL can identify groups of correlated variables but  it forces the model to be constant within a group.  This artifact induces unnecessary bias in the model estimation.  In this paper we take a submodular perspective and show that  OWL can be posed as the Lov{\'a}sz extension of a suitably defined submodular function. The submodular perspective not only explains the group-wise constant behavior of OWL, but also suggests alternatives. The main contribution of this paper is smoothed OWL (SOWL), a new family of norms, which  not only identifies the groups but also allows the model to be flexible inside a group.   We establish several algorithmic and theoretical properties of SOWL including group identification and model consistency. We also provide algorithmic tools to compute the SOWL norm and its proximal operator, whose computational complexity  $O(d\log d)$ is significantly better than that of general purpose solvers in $O(d^2\log d)$. In our experiments, SOWL compares favorably with respect to OWL in the regimes of interest.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Raman Sankaran and Francis Bach and Chiranjib Bhattacharya},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:23:23 +0000},
	Date-Modified = {2017-08-13 15:23:23 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {1123--1131},
	Pdf = {http://proceedings.mlr.press/v54/sankaran17a/sankaran17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Identifying Groups of Strongly Correlated Variables through Smoothed Ordered Weighted $L_1$-norms}},
	Url = {http://proceedings.mlr.press/v54/sankaran17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/sankaran17a.html}}

@inproceedings{pmlr-v54-mehta17a,
	Abstract = {We present an algorithm for the statistical learning setting with a bounded exp-concave loss in d dimensions that obtains excess risk $O(d \log(1/δ)/n)$ with probability $1 - δ$. The core technique is to boost the confidence of recent in-expectation O(d/n) excess risk bounds for empirical risk minimization (ERM), without sacrificing the rate, by leveraging a Bernstein condition which holds due to exp-concavity.  We also show that a regret bound for any online learner in this setting translates to a high probability excess risk bound for the corresponding online-to-batch conversion of the online learner.  Lastly, we present high probability bounds for the exp-concave model selection aggregation problem that are quantile-adaptive in a certain sense. One bound obtains a nearly optimal rate without requiring the loss to be Lipschitz continuous, and another requires Lipschitz continuity but obtains the optimal rate.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Nishant Mehta},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:22:11 +0000},
	Date-Modified = {2017-08-13 15:22:11 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {1085--1093},
	Pdf = {http://proceedings.mlr.press/v54/mehta17a/mehta17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Fast rates with high probability in exp-concave statistical learning}},
	Url = {http://proceedings.mlr.press/v54/mehta17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/mehta17a.html}}

@inproceedings{pmlr-v54-moscovich17a,
	Abstract = {We consider semi-supervised regression when the predictor variables are drawn from an unknown manifold. A simple two step approach to this problem is to: (i) estimate the manifold geodesic distance between any pair of points using both the labeled and unlabeled instances; and (ii) apply a k nearest neighbor regressor based on these distance estimates. We prove that given sufficiently many unlabeled points, this simple method of geodesic kNN regression achieves the optimal finite-sample minimax bound on the mean squared error, as if the manifold were known. Furthermore, we show how this approach can be efficiently implemented, requiring only O(k N log N) operations to estimate the regression function at all N labeled and unlabeled points. We illustrate this approach on two datasets with a manifold structure: indoor localization using WiFi fingerprints and facial pose estimation. In both cases, geodesic kNN is more accurate and much faster than the popular Laplacian eigenvector regressor.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Amit Moscovich and Ariel Jaffe and Nadler Boaz},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:21:24 +0000},
	Date-Modified = {2017-08-13 15:21:24 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {933--942},
	Pdf = {http://proceedings.mlr.press/v54/moscovich17a/moscovich17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Minimax-optimal semi-supervised regression on unknown manifolds}},
	Url = {http://proceedings.mlr.press/v54/moscovich17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/moscovich17a.html}}

@inproceedings{pmlr-v54-lattimore17a,
	Abstract = {Stochastic linear bandits are a natural and simple generalisation of finite-armed bandits with numerous practical applications. Current approaches focus on generalising existing techniques for finite-armed bandits, notably the otimism principle and Thompson sampling. Prior analysis has mostly focussed on the worst-case setting. We analyse the asymptotic regret and show matching upper and lower bounds on what is achievable. Surprisingly, our results show that no algorithm based on optimism or Thompson sampling will ever achieve the optimal rate. In fact, they can be arbitrarily far from optimal, even in very simple cases. This is a disturbing result because these techniques are standard tools that are widely used for sequential optimisation, for example,  generalised linear bandits and reinforcement learning. },
	Address = {Fort Lauderdale, FL, USA},
	Author = {Tor Lattimore and Csaba Szepesvari},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:19:42 +0000},
	Date-Modified = {2017-08-13 15:19:42 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {728--737},
	Pdf = {http://proceedings.mlr.press/v54/lattimore17a/lattimore17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{The End of Optimism? An Asymptotic Analysis of Finite-Armed Linear Bandits}},
	Url = {http://proceedings.mlr.press/v54/lattimore17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/lattimore17a.html}}

@inproceedings{pmlr-v54-bahmani17a,
	Abstract = {We propose a flexible convex relaxation for the phase retrieval problem that operates in the natural domain of the signal. Therefore, we avoid the prohibitive computational cost associated with ``lifting'' and semidefinite programming (SDP) in methods such as PhaseLift and compete with recently developed non-convex techniques for phase retrieval. We relax the quadratic equations for phaseless measurements to inequality constraints each of which representing a symmetric ``slab''. Through a simple convex program, our proposed estimator finds an extreme point of the intersection of these slabs that is best aligned with a given anchor vector. We characterize geometric conditions that certify success of the proposed estimator. Furthermore, using classic results in statistical learning theory, we show that for random measurements the geometric certificates hold with high probability at an optimal sample complexity. Phase transition of our estimator is evaluated through simulations. Our numerical experiments also suggest that the proposed method can solve phase retrieval problems with coded diffraction measurements as well.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Sohail Bahmani and Justin Romberg},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:15:15 +0000},
	Date-Modified = {2017-08-13 15:15:15 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {252--260},
	Pdf = {http://proceedings.mlr.press/v54/bahmani17a/bahmani17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Phase Retrieval Meets Statistical Learning Theory: A Flexible Convex Relaxation}},
	Url = {http://proceedings.mlr.press/v54/bahmani17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/bahmani17a.html}}

@inproceedings{pmlr-v54-zimin17a,
	Abstract = {In this work we study the learnability of stochastic processes with respect to the conditional risk, i.e. the existence of a learning algorithm that improves its next-step performance with the amount of observed data.  We introduce a notion of pairwise discrepancy between conditional distributions at different times steps and show how certain properties of these discrepancies can be used to construct a successful learning algorithm. Our main results are two theorems that establish criteria for learnability for many classes of stochastic processes, including all special cases studied previously in the literature. },
	Address = {Fort Lauderdale, FL, USA},
	Author = {Alexander Zimin and Christoph Lampert},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:14:02 +0000},
	Date-Modified = {2017-08-13 15:14:02 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {213--222},
	Pdf = {http://proceedings.mlr.press/v54/zimin17a/zimin17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Learning Theory for Conditional Risk Minimization}},
	Url = {http://proceedings.mlr.press/v54/zimin17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/zimin17a.html}}

@inproceedings{pmlr-v54-sasaki17a,
	Abstract = {Estimation of \emphdensity ridges has been gathering a great deal of attention since it enables us to reveal lower-dimensional structures hidden in data. Recently, \emphsubspace constrained mean shift (SCMS) was proposed as a practical algorithm for density ridge estimation. A key technical ingredient in SCMS is to accurately estimate the ratios of the density derivatives to the density. SCMS takes a three-step approach for this purpose --- first estimating the data density, then computing its derivatives, and finally taking their ratios. However, this three-step approach can be unreliable because a good density estimator does not necessarily mean a good density derivative estimator and division by an estimated density could significantly magnify the estimation error. To overcome these problems, we propose a novel method that directly estimates the ratios without going through density estimation and division. Our proposed estimator has an analytic-form solution and it can be computed efficiently. We further establish a non-parametric convergence bound for the proposed ratio estimator. Finally, based on this direct ratio estimator, we develop a practical algorithm for density ridge estimation and experimentally demonstrate its usefulness on a variety of datasets.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Hiroaki Sasaki and Takafumi Kanamori and Masashi Sugiyama},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:12:50 +0000},
	Date-Modified = {2017-08-13 15:12:50 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {204--212},
	Pdf = {http://proceedings.mlr.press/v54/sasaki17a/sasaki17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Estimating Density Ridges by Direct Estimation of Density-Derivative-Ratios}},
	Url = {http://proceedings.mlr.press/v54/sasaki17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/sasaki17a.html}}

@inproceedings{pmlr-v54-alabdulmohsin17a,
	Abstract = {One fundamental goal in any learning algorithm is to mitigate its risk for overfitting. Mathematically, this requires that the learning algorithm enjoys a small generalization risk, which is defined either in expectation or in probability. Both types of generalization are commonly used in the literature. For instance, generalization in expectation has been used to analyze algorithms, such as ridge regression and SGD, whereas generalization in probability is used in the VC theory, among others. Recently, a third notion of generalization has been studied, called uniform generalization, which requires that the generalization risk vanishes uniformly in expectation across all bounded parametric losses. It has been shown that uniform generalization is, in fact, equivalent to an information-theoretic stability constraint, and that it recovers classical results in learning theory. It is achievable under various settings, such as sample compression schemes, finite hypothesis spaces, finite domains, and differential privacy. However, the relationship between uniform generalization and concentration remained unknown. In this paper, we answer this question by proving that, while a generalization in expectation does not imply a generalization in probability, a uniform generalization in expectation does imply concentration. We establish a chain rule for the uniform generalization risk of the composition of hypotheses and use it to derive a large deviation bound. Finally, we prove that the bound is tight.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Ibrahim Alabdulmohsin},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:08:34 +0000},
	Date-Modified = {2017-08-13 15:08:34 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {92--100},
	Pdf = {http://proceedings.mlr.press/v54/alabdulmohsin17a/alabdulmohsin17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{An Information-Theoretic Route from Generalization in Expectation to Generalization in Probability}},
	Url = {http://proceedings.mlr.press/v54/alabdulmohsin17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/alabdulmohsin17a.html}}

@inproceedings{pmlr-v54-scarlett17a,
	Abstract = {We consider the problem of estimating the underlying graph associated with a Markov random field, with the added twist that the decoding algorithm can iteratively choose which subsets of nodes to sample based on the previous samples, resulting in an active learning setting.  Considering both Ising and Gaussian models, we provide algorithm-independent lower bounds for high-probability recovery within the class of degree-bounded graphs.  Our main results are minimax lower bounds for the active setting that match the best known lower bounds for the passive setting, which in turn are known to be tight in several cases of interest.     Our analysis is based on Fano's inequality, along with novel mutual information bounds for the active learning setting, and the application of restricted graph ensembles.  While we consider ensembles that are similar or identical to those used in the passive setting, we require different analysis techniques, with a key challenge being bounding a mutual information quantity associated with observed subsets of nodes, as opposed to full observations.},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Jonathan Scarlett and Volkan Cevher},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:07:11 +0000},
	Date-Modified = {2017-08-13 15:07:11 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {55--64},
	Pdf = {http://proceedings.mlr.press/v54/scarlett17a/scarlett17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Lower Bounds on Active Learning for Graphical Model Selection}},
	Url = {http://proceedings.mlr.press/v54/scarlett17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/scarlett17a.html}}

@inproceedings{pmlr-v54-li17a,
	Abstract = {We present minimax bounds for classification and clustering error in the setting where covariates are drawn from a mixture of two isotropic Gaussian distributions. Here, we define clustering error in a  discriminative fashion, demonstrating fundamental connections between classification (supervised) and clustering (unsupervised). For both classification and clustering, our lower bounds show that without enough samples, the best any classifier or clustering rule can do is close to random guessing. For classification, as part of our upper bound analysis,  we show that Fisher's linear discriminant achieves a fast minimax rate $\Theta(1/n)$ with enough samples $n$. For clustering, as part of our upper bound analysis, we show that a clustering rule constructed using principal component analysis achieves the minimax rate with enough samples. We also provide lower and upper bounds for the high-dimensional sparse  setting where the dimensionality of the covariates $p$ is potentially larger than the number of samples $n$, but where the difference between the Gaussian means is sparse.
},
	Address = {Fort Lauderdale, FL, USA},
	Author = {Tianyang Li and Xinyang Yi and Constantine Carmanis and Pradeep Ravikumar},
	Booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
	Date-Added = {2017-08-13 15:01:48 +0000},
	Date-Modified = {2017-08-13 15:03:23 +0000},
	Editor = {Aarti Singh and Jerry Zhu},
	Month = {20--22 Apr},
	Pages = {1--9},
	Pdf = {http://proceedings.mlr.press/v54/li17a/li17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{Minimax Gaussian Classification \& Clustering}},
	Url = {http://proceedings.mlr.press/v54/li17a.html},
	Volume = {54},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v54/li17a.html}}

@inproceedings{pmlr-v70-yen17a,
	Abstract = {The latent feature model (LFM), proposed in (Griffiths \& Ghahramani, 2005), but possibly with earlier origins, is a generalization of a mixture model, where each instance is generated not from a single latent class but from a combination of \emph{latent features}. Thus, each instance has an associated latent binary feature incidence vector indicating the presence or absence of a feature. Due to its combinatorial nature, inference of LFMs is considerably intractable, and accordingly, most of the attention has focused on nonparametric LFMs, with priors such as the Indian Buffet Process (IBP) on infinite binary matrices. Recent efforts to tackle this complexity either still have computational complexity that is exponential, or sample complexity that is high-order polynomial w.r.t. the number of latent features. In this paper, we address this outstanding problem of tractable estimation of LFMs via a novel atomic-norm regularization, which gives an algorithm with polynomial run-time and sample complexity without impractical assumptions on the data distribution.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Ian En-Hsu Yen and Wei-Cheng Lee and Sung-En Chang and Arun Sai Suggala and Shou-De Lin and Pradeep Ravikumar},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3949--3957},
	Pdf = {http://proceedings.mlr.press/v70/yen17a/yen17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Latent Feature Lasso},
	Url = {http://proceedings.mlr.press/v70/yen17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/yen17a.html}}

@inproceedings{pmlr-v70-ravanbakhsh17a,
	Abstract = {We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group G that acts discretely on the input and output of a standard neural network layer, we show that its equivariance is linked to the symmetry group of network parameters. We then propose two parameter-sharing scheme to induce the desirable symmetry on the parameters of the neural network. Under some conditions on the action of G, our procedure for tying the parameters achieves G-equivariance and guarantees sensitivity to all other permutation groups outside of G.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Siamak Ravanbakhsh and Jeff Schneider and Barnab{\'a}s P{\'o}czos},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {2892--2901},
	Pdf = {http://proceedings.mlr.press/v70/ravanbakhsh17a/ravanbakhsh17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Equivariance Through Parameter-Sharing},
	Url = {http://proceedings.mlr.press/v70/ravanbakhsh17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/ravanbakhsh17a.html}}

@inproceedings{pmlr-v70-lei17b,
	Abstract = {We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations. With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy Primal-Dual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables. It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse. We then extend this algorithm further to leverage active sets. The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Qi Lei and Ian En-Hsu Yen and Chao-yuan Wu and Inderjit S. Dhillon and Pradeep Ravikumar},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {2034--2042},
	Pdf = {http://proceedings.mlr.press/v70/lei17b/lei17b.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization},
	Url = {http://proceedings.mlr.press/v70/lei17b.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/lei17b.html}}

@inproceedings{pmlr-v70-krishnamurthy17a,
	Abstract = {We design an active learning algorithm for cost-sensitive multiclass classification: problems where different errors have different costs. Our algorithm, COAL, makes predictions by regressing to each label's cost and predicting the smallest. On a new example, it uses a set of regressors that perform well on past data to estimate possible costs for each label. It queries only the labels that could be the best, ignoring the sure losers. We prove COAL can be efficiently implemented for any regression family that admits squared loss optimization; it also enjoys strong guarantees with respect to predictive performance and labeling effort. Our experiment with COAL show significant improvements in labeling effort and test cost over passive and active baselines.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Akshay Krishnamurthy and Alekh Agarwal and Tzu-Kuo Huang and Daum{\'e}, III, Hal and John Langford},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {1915--1924},
	Pdf = {http://proceedings.mlr.press/v70/krishnamurthy17a/krishnamurthy17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Active Learning for Cost-Sensitive Classification},
	Url = {http://proceedings.mlr.press/v70/krishnamurthy17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/krishnamurthy17a.html}}

@inproceedings{pmlr-v70-kale17a,
	Abstract = {Online sparse linear regression is an online problem where an algorithm repeatedly chooses a subset of coordinates to observe in an adversarially chosen feature vector, makes a real-valued prediction, receives the true label, and incurs the squared loss. The goal is to design an online learning algorithm with sublinear regret to the best sparse linear predictor in hindsight. Without any assumptions, this problem is known to be computationally intractable. In this paper, we make the assumption that data matrix satisfies restricted isometry property, and show that this assumption leads to computationally efficient algorithms with sublinear regret for two variants of the problem. In the first variant, the true label is generated according to a sparse linear model with additive Gaussian noise. In the second, the true label is chosen adversarially.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Satyen Kale and Zohar Karnin and Tengyuan Liang and D{\'a}vid P{\'a}l},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {1780--1788},
	Pdf = {http://proceedings.mlr.press/v70/kale17a/kale17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Adaptive Feature Selection: Computationally Efficient Online Sparse Linear Regression under {RIP}},
	Url = {http://proceedings.mlr.press/v70/kale17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/kale17a.html}}

@inproceedings{pmlr-v70-loukas17a,
	Abstract = {How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply \emph{non-asymptotic} concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from $O(1)$ samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Andreas Loukas},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {2228--2237},
	Pdf = {http://proceedings.mlr.press/v70/loukas17a/loukas17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?},
	Url = {http://proceedings.mlr.press/v70/loukas17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/loukas17a.html}}

@inproceedings{pmlr-v70-wang17f,
	Abstract = {We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines. In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted $\ell_1$ regularized loss minimization problem. After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Jialei Wang and Mladen Kolar and Nathan Srebro and Tong Zhang},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3636--3645},
	Pdf = {http://proceedings.mlr.press/v70/wang17f/wang17f.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Efficient Distributed Learning with Sparsity},
	Url = {http://proceedings.mlr.press/v70/wang17f.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/wang17f.html}}

@inproceedings{pmlr-v70-singh17a,
	Abstract = {We study the problem of using i.i.d. samples from an unknown multivariate probability distribution p to estimate the mutual information of p. This problem has recently received attention in two settings: (1) where p is assumed to be Gaussian and (2) where p is assumed only to lie in a large nonparametric smoothness class. Estimators proposed for the Gaussian case converge in high dimensions when the Gaussian assumption holds, but are brittle, failing dramatically when p is not Gaussian, while estimators proposed for the nonparametric case fail to converge with realistic sample sizes except in very low dimension. Hence, there is a lack of robust mutual information estimators for many realistic data. To address this, we propose estimators for mutual information when p is assumed to be a nonparanormal (or Gaussian copula) model, a semiparametric compromise between Gaussian and nonparametric extremes. Using theoretical bounds and experiments, we show these estimators strike a practical balance between robustness and scalability.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Shashank Singh and Barnab{\'a}s P{\'o}czos},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3210--3219},
	Pdf = {http://proceedings.mlr.press/v70/singh17a/singh17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Nonparanormal Information Estimation},
	Url = {http://proceedings.mlr.press/v70/singh17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/singh17a.html}}

@inproceedings{pmlr-v70-wang17c,
	Abstract = {We address the statistical and optimization impacts of using classical sketch versus Hessian sketch to solve approximately the Matrix Ridge Regression (MRR) problem. Prior research has considered the effects of classical sketch on least squares regression (LSR), a strictly simpler problem. We establish that classical sketch has a similar effect upon the optimization properties of MRR as it does on those of LSR---namely, it recovers nearly optimal solutions. In contrast, Hessian sketch does not have this guarantee; instead, the approximation error is governed by a subtle interplay between the ``mass'' in the responses and the optimal objective value. For both types of approximations, the regularization in the sketched MRR problem gives it significantly different statistical properties from the sketched LSR problem. In particular, there is a bias-variance trade-off in sketched MRR that is not present in sketched LSR. We provide upper and lower bounds on the biases and variances of sketched MRR; these establish that the variance is significantly increased when classical sketches are used, while the bias is significantly increased when using Hessian sketches. Empirically, sketched MRR solutions can have risks that are higher by an order-of-magnitude than those of the optimal MRR solutions. We establish theoretically and empirically that model averaging greatly decreases this gap. Thus, in the distributed setting, sketching combined with model averaging is a powerful technique that quickly obtains near-optimal solutions to the MRR problem while greatly mitigating the statistical risks incurred by sketching.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Shusen Wang and Alex Gittens and Michael W. Mahoney},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3608--3616},
	Pdf = {http://proceedings.mlr.press/v70/wang17c/wang17c.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Sketched Ridge Regression: Optimization Perspective, Statistical Perspective, and Model Averaging},
	Url = {http://proceedings.mlr.press/v70/wang17c.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/wang17c.html}}

@inproceedings{pmlr-v70-sheffet17a,
	Abstract = {Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives t-values --- representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for well-spread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with $l_2$-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the ``Analyze Gauss'' algorithm (Dwork et al 2014).},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Or Sheffet},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3105--3114},
	Pdf = {http://proceedings.mlr.press/v70/sheffet17a/sheffet17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Differentially Private Ordinary Least Squares},
	Url = {http://proceedings.mlr.press/v70/sheffet17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/sheffet17a.html}}

@inproceedings{pmlr-v70-shalit17a,
	Abstract = {There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a ``balanced'' representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Uri Shalit and Fredrik D. Johansson and David Sontag},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {3076--3085},
	Pdf = {http://proceedings.mlr.press/v70/shalit17a/shalit17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Estimating individual treatment effect: generalization bounds and algorithms},
	Url = {http://proceedings.mlr.press/v70/shalit17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/shalit17a.html}}

@inproceedings{pmlr-v70-jitkrittum17a,
	Abstract = {A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratic-time HSIC test, and outperform competing O(n) and O(n log n) tests.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Wittawat Jitkrittum and Zolt{\'a}n Szab{\'o} and Arthur Gretton},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {1742--1751},
	Pdf = {http://proceedings.mlr.press/v70/jitkrittum17a/jitkrittum17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {An Adaptive Test of Independence with Analytic Kernel Embeddings},
	Url = {http://proceedings.mlr.press/v70/jitkrittum17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/jitkrittum17a.html}}

@inproceedings{pmlr-v70-gilmer17a,
	Abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {1263--1272},
	Pdf = {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {Neural Message Passing for Quantum Chemistry},
	Url = {http://proceedings.mlr.press/v70/gilmer17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/gilmer17a.html}}

@inproceedings{pmlr-v70-oglic17a,
	Abstract = {We investigate, theoretically and empirically, the effectiveness of kernel K-means++ samples as landmarks in the Nystr{\"o}m method for low-rank approximation of kernel matrices. Previous empirical studies (Zhang et al., 2008; Kumar et al.,2012) observe that the landmarks obtained using (kernel) K-means clustering define a good low-rank approximation of kernel matrices. However, the existing work does not provide a theoretical guarantee on the approximation error for this approach to landmark selection. We close this gap and provide the first bound on the approximation error of the Nystr{\"o}m method with kernel K-means++ samples as landmarks. Moreover, for the frequently used Gaussian kernel we provide a theoretically sound motivation for performing Lloyd refinements of kernel K-means++ landmarks in the instance space. We substantiate our theoretical results empirically by comparing the approach to several state-of-the-art algorithms.},
	Address = {International Convention Centre, Sydney, Australia},
	Author = {Dino Oglic and Thomas G{\"a}rtner},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning},
	Editor = {Doina Precup and Yee Whye Teh},
	Month = {06--11 Aug},
	Pages = {2652--2660},
	Pdf = {http://proceedings.mlr.press/v70/oglic17a/oglic17a.pdf},
	Publisher = {PMLR},
	Series = {Proceedings of Machine Learning Research},
	Title = {{N}ystr{\"o}m Method with Kernel K-means++ Samples as Landmarks},
	Url = {http://proceedings.mlr.press/v70/oglic17a.html},
	Volume = {70},
	Year = {2017},
	Bdsk-Url-1 = {http://proceedings.mlr.press/v70/oglic17a.html}}
